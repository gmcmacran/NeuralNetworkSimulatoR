---
title: "Custom Activation"
output: html_document
vignette: >
  %\VignetteIndexEntry{CustomActivation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The package is designed to be easily extended. In this vignette, a custom activation will be used. Please read the README and and Deep Regression Data vignette before this one.

## Defining A Custom Activation

The usual relu is linear if x is above 0 with no maximum. The custom activation will be linear between 1 and 10. THe maximum value will be 10. 

```{r p1}
custom_relu_R <- function(x) {
  out <- ifelse(x >= 1, x, 1)
  out <- pmin(out, 20)
  
  return(out)
}

```

The above just a regular R function. Lets test a few values to make sure it acts as expected.

```{r p2}
custom_relu_R(0)
custom_relu_R(1)
custom_relu_R(5)
custom_relu_R(10)
custom_relu_R(20)
custom_relu_R(22)
```

## Making Data

With the function defined, the remaining code is nearly identical to the README example. Only the activation changes.

```{r data1}
library(dplyr)
library(NeuralNetworkSimulatoR)

M <- list(matrix(c(-1, -2, -3), nrow = 3, ncol = 1))
A <- list(custom_relu_R)
```

```{r r data2}
set.seed(1)
simData <- simulate_regression_data(
  rows = 1000L,
  N = 3L, U = 0L, C = 0L,
  matrices = M, activations = A,
  noise = 0
)

head(simData)
```

```{r r data3}
library(keras)

X <- simData[,1:3]
Y <- simData[,4]


model <- keras_model_sequential() %>%
  layer_dense(units = 1, activation = "relu", input_shape = 3, use_bias = FALSE, kernel_initializer = initializer_constant(value = 1))

model %>%
  compile(loss = loss_mean_squared_error,
          optimizer = optimizer_rmsprop(lr = .0001))

model %>%
  fit(x = X, y = Y,
      epochs = 20,
      batch_size = 10,
      verbose = FALSE)

get_weights(model)

```
