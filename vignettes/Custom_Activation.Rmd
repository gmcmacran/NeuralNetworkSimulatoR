---
title: "Custom Activation"
output: html_document
vignette: >
  %\VignetteIndexEntry{CustomActivation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The package is designed to be easily extended. In this vignette, a custom activation will be used. Please read the README and Deep Regression Data vignette before this one.

## Defining A Custom Activation

The usual relu is linear if x is above zero with no maximum. The custom activation will be linear starting at one instead of zero and will have a maximum value of ten. Note this is just a regular R function.

```{r p1}
custom_relu_R <- function(x) {
  out <- ifelse(x >= 1, x, 1)
  out <- pmin(out, 20)
  
  return(out)
}

```

## Making Data

With the custom activation defined, the remaining code is nearly identical to the README example. Only the activation changes.

```{r data1}
library(dplyr, warn.conflicts = FALSE)
library(NeuralNetworkSimulatoR)

M <- list(matrix(c(-1, -2, -3), nrow = 3, ncol = 1))
A <- list(custom_relu_R)
```

```{r r data2}
set.seed(1)
simData <- simulate_regression_data(
  rows = 1000L,
  N = 3L, U = 0L, C = 0L,
  matrices = M, activations = A,
  noise = 0
)

head(simData)
```

```{r LR3}
library(keras)

X <- simData[,1:3]
Y <- simData[,4]


model <- keras_model_sequential() %>%
  layer_dense(input_shape = 3, 
              use_bias = FALSE, 
              kernel_initializer = initializer_constant(value = 1),
              units = 1) %>%
  activation_relu(alpha = 1, threshold = 10)

model %>%
  compile(loss = loss_mean_squared_error,
          optimizer = optimizer_rmsprop())

model %>%
  fit(x = X, y = Y,
      epochs = 30,
      batch_size = 10,
      verbose = FALSE)

get_weights(model)

```
